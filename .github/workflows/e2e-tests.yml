# .github/workflows/e2e-tests.yml
name: E2E Tests

on:
  push:
    branches: [main]
    paths-ignore:
      - "**.md"
      - "docs/**"
      - "screenshots/**"
      - "LICENSE"
      - ".gitignore"
  workflow_dispatch:
    inputs:
      shard_count:
        description: "Number of test shards (1-8)"
        required: false
        default: "4"
        type: choice
        options:
          - "1"
          - "2"
          - "4"
          - "8"

# Cancel in-progress runs when a new commit is pushed
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  test:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    permissions:
      contents: read
      pull-requests: write
      checks: write

    env:
      DATABASE_URL: postgresql://test_user:test_password@localhost:5433/test_timetable
      # Disable Playwright global-setup Docker compose in CI; we use GitHub service container
      AUTO_MANAGE_TEST_DB: "false"

    # Test sharding: Disabled for sequential stability - run all tests in single job
    strategy:
      fail-fast: false
      matrix:
        shard: [1]  # Single shard for stability (was [1, 2, 3, 4])

    # Temporary PostgreSQL service container
    services:
      postgres:
        image: postgres:16
        env:
          POSTGRES_USER: test_user
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: test_timetable
        ports:
          - 5433:5432
        options: >-
          --health-cmd "pg_isready -U test_user -d test_timetable"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "pnpm"

      - name: Cache pnpm store & Next.js build cache
        uses: actions/cache@v4
        with:
          path: |
            ~/.pnpm-store
            ~/.cache/pnpm
            ~/.local/share/pnpm
            .next/cache
          key: ${{ runner.os }}-nextjs-v2-${{ hashFiles('**/pnpm-lock.yaml') }}-${{ hashFiles('**/*.js', '**/*.jsx', '**/*.ts', '**/*.tsx') }}
          restore-keys: |
            ${{ runner.os }}-nextjs-v2-${{ hashFiles('**/pnpm-lock.yaml') }}-
            ${{ runner.os }}-nextjs-v2-

      # Setup environment variables for E2E testing
      - name: Setup environment variables
        run: |
          # Write all required env files so dev server + build share the same config
          cat > .env.test << EOF
          DATABASE_URL=postgresql://test_user:test_password@localhost:5433/test_timetable
          BETTER_AUTH_SECRET=${{ secrets.BETTER_AUTH_SECRET }}
          BETTER_AUTH_URL=${{ vars.BETTER_AUTH_URL }}
          AUTH_GOOGLE_ID=dummy-client-id
          AUTH_GOOGLE_SECRET=dummy-client-secret
          SEED_FOR_TESTS=true
          SEED_CLEAN_DATA=true
          NEXT_TELEMETRY_DISABLED=1
          NODE_ENV=test
          CI=true
          EOF
          cat > .env.test.local << EOF
          DATABASE_URL=postgresql://test_user:test_password@localhost:5433/test_timetable
          BETTER_AUTH_SECRET=${{ secrets.BETTER_AUTH_SECRET }}
          BETTER_AUTH_URL=${{ vars.BETTER_AUTH_URL }}
          AUTH_GOOGLE_ID=dummy-client-id
          AUTH_GOOGLE_SECRET=dummy-client-secret
          SEED_FOR_TESTS=true
          SEED_CLEAN_DATA=true
          NEXT_TELEMETRY_DISABLED=1
          NODE_ENV=test
          CI=true
          EOF
          echo ".env.test contents:" && sed -e 's/=.*/=[REDACTED]/' .env.test
          echo ".env.test.local contents:" && sed -e 's/=.*/=[REDACTED]/' .env.test.local
          cp .env.test .env.local
          cp .env.test .env.production
          echo "Synced .env.test -> .env.local & .env.production"

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Generate Prisma Client
        run: pnpm prisma generate

      - name: Apply database migrations
        run: pnpm prisma migrate deploy

      # Seed test database with MOE-compliant data
      # SEED_FOR_TESTS=true triggers:
      # - E2E test data seeding (56 teachers, 82 subjects, etc.)
      # - Auth session cleanup (prevents stale session conflicts)
      # - Verification token cleanup
      - name: Seed test database
        run: pnpm db:seed:clean
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5433/test_timetable
          BETTER_AUTH_SECRET: ${{ secrets.BETTER_AUTH_SECRET }}
          BETTER_AUTH_URL: ${{ vars.BETTER_AUTH_URL }}
          SEED_FOR_TESTS: "true"
          SEED_CLEAN_DATA: "true"
      - name: Project build flow (Next.js production build)
        run: pnpm build
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5433/test_timetable
          BETTER_AUTH_SECRET: ${{ secrets.BETTER_AUTH_SECRET }}
          BETTER_AUTH_URL: ${{ vars.BETTER_AUTH_URL }}
          AUTH_GOOGLE_ID: dummy-client-id
          AUTH_GOOGLE_SECRET: dummy-client-secret
          SEED_FOR_TESTS: "true"
          SEED_CLEAN_DATA: "true"
          NEXT_TELEMETRY_DISABLED: "1"
          NODE_ENV: production
          # Sentry source map upload (optional - builds succeed without it)
          SENTRY_AUTH_TOKEN: ${{ secrets.SENTRY_AUTH_TOKEN }}

      - name: Start Next.js production server
        run: pnpm start > server.log 2>&1 &
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5433/test_timetable
          BETTER_AUTH_SECRET: ${{ secrets.BETTER_AUTH_SECRET }}
          BETTER_AUTH_URL: ${{ vars.BETTER_AUTH_URL }}
          AUTH_GOOGLE_ID: dummy-client-id
          AUTH_GOOGLE_SECRET: dummy-client-secret
          NEXT_TELEMETRY_DISABLED: "1"
          PORT: 3000
          CI: "true"
          DEBUG_MODE: "true"
          # Explicitly set NODE_ENV for rate limiting logic in auth.ts
          NODE_ENV: production

      - name: Wait for server to be ready
        run: |
          echo "Waiting for Next.js server to start on http://localhost:3000..."
          timeout 60 bash -c 'until curl -f http://localhost:3000 > /dev/null 2>&1; do 
            echo "Server not ready yet, retrying in 2s..."
            sleep 2
          done'
          echo "âœ… Server is ready!"

      # Verify database is fully seeded before running E2E tests
      # Prevents race conditions where tests start before data is available
      # Related: Issue #172, Commit 50f6861
      - name: Verify database is ready for tests
        run: |
          echo "ðŸ” Verifying database seed status..."
          max_attempts=30
          attempt=1
          wait_time=2

          while [ $attempt -le $max_attempts ]; do
            echo "â³ Attempt $attempt/$max_attempts - Checking database health..."
            
            # Check if health endpoint responds and database is ready
            if curl -f http://localhost:3000/api/health/db 2>/dev/null | jq -e '.ready == true' > /dev/null 2>&1; then
              echo "âœ… Database is ready!"
              echo "ðŸ“Š Database health status:"
              curl -s http://localhost:3000/api/health/db | jq .
              break
            fi
            
            if [ $attempt -eq $max_attempts ]; then
              echo "âŒ Database failed to be ready after $max_attempts attempts"
              echo "ðŸ“Š Final health check response:"
              curl -s http://localhost:3000/api/health/db 2>/dev/null | jq . || echo "Health endpoint not responding"
              echo ""
              echo "ðŸ” Diagnostic Information:"
              echo "Expected minimum counts: teachers=8, schedules=30, timeslots=80"
              echo ""
              echo "ðŸ’¡ This usually means:"
              echo "   1. Database seed did not complete successfully"
              echo "   2. Application is not connecting to the correct database"
              echo "   3. Prisma migrations were not applied"
              exit 1
            fi
            
            echo "   Not ready yet, waiting ${wait_time}s..."
            sleep $wait_time
            attempt=$((attempt + 1))
          done
        env:
          NODE_ENV: test

      - name: Install Playwright browsers
        run: pnpm exec playwright install --with-deps chromium

      - name: Run E2E tests (Shard ${{ matrix.shard }}/4)
        run: pnpm exec playwright test --shard=${{ matrix.shard }}/4
        env:
          DATABASE_URL: postgresql://test_user:test_password@localhost:5433/test_timetable
          BETTER_AUTH_SECRET: ${{ secrets.BETTER_AUTH_SECRET }}
          BETTER_AUTH_URL: ${{ vars.BETTER_AUTH_URL }}
          AUTH_GOOGLE_ID: dummy-client-id
          AUTH_GOOGLE_SECRET: dummy-client-secret
          NEXT_TELEMETRY_DISABLED: "1"
          CI: "true"
          PLAYWRIGHT_JUNIT_OUTPUT_FILE: test-results/results-${{ matrix.shard }}.xml
          # Ensure global-setup respects CI-provided Postgres service
          AUTO_MANAGE_TEST_DB: "false"
          # Skip Playwright global-setup seed; database already seeded by workflow
          SKIP_DB_SEED: "true"

      - name: Display server logs on failure
        if: failure()
        run: |
          echo "=== Server Logs (Last 200 lines) ==="
          if [ -f server.log ]; then
            tail -200 server.log
          else
            echo "No server.log file found"
          fi

      - name: Upload Playwright JSON results and artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-shard-${{ matrix.shard }}
          path: |
            test-results/
            artifacts/
          retention-days: 14

      - name: Upload Playwright HTML report (per shard)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-html-shard-${{ matrix.shard }}
          path: playwright-report/
          retention-days: 14

  # Merge reports from all shards
  merge-reports:
    if: always()
    needs: [test]
    runs-on: ubuntu-latest

    env:
      DATABASE_URL: postgresql://dummy:dummy@localhost:5432/dummy

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup pnpm
        uses: pnpm/action-setup@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: "20"
          cache: "pnpm"

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Download shard artifacts
        uses: actions/download-artifact@v4
        with:
          path: merged-results
          pattern: test-results-shard-*

      - name: Download HTML report shards
        uses: actions/download-artifact@v4
        with:
          path: html-shards
          pattern: playwright-html-shard-*

      - name: Install Playwright for report merge
        run: pnpm exec playwright install --with-deps chromium

      - name: Merge HTML reports
        run: |
          mkdir -p playwright-report
          # Merge all shard blob reports into a single HTML report
          # The merge-reports command expects blob files (.zip), not HTML folders
          # First, check if we have any blob files to merge
          if ls html-shards/*/report.zip 1> /dev/null 2>&1; then
            pnpm exec playwright merge-reports --reporter=html "html-shards/*"
          elif [ -d "html-shards" ] && [ "$(ls -A html-shards)" ]; then
            # Fallback: copy first shard's HTML report if no blobs
            cp -r html-shards/playwright-html-shard-1/* playwright-report/ 2>/dev/null || true
          fi

      - name: Combine JSON results (simple concat)
        run: |
          mkdir -p playwright-report
          node -e "const fs=require('fs');const path='merged-results';const files=fs.readdirSync(path).flatMap(d=>{const p=path+'/'+d+'/test-results/results.json';return fs.existsSync(p)?[p]:[]});const all=files.map(f=>JSON.parse(fs.readFileSync(f,'utf8')));fs.writeFileSync('playwright-report/merged-results.json', JSON.stringify(all,null,2));"

      - name: Collect Playwright JUnit XML results
        run: |
          mkdir -p playwright-report/junit
          shopt -s globstar nullglob
          files=(merged-results/**/results-*.xml)
          if [ ${#files[@]} -eq 0 ]; then
            echo "No Playwright JUnit XML files found"
          else
            for file in "${files[@]}"; do
              cp "$file" "playwright-report/junit/$(basename "$file")"
            done
          fi

      - name: Upload merged JSON results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-merged-json
          path: playwright-report/merged-results.json
          retention-days: 30

      - name: Upload merged Playwright JUnit results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-junit
          path: playwright-report/junit
          retention-days: 30

      - name: Upload merged HTML report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-html-report
          path: playwright-report/
          retention-days: 30
# Temporary database is automatically destroyed after job completes
# Test sharding: 4 parallel jobs by default, customizable via workflow_dispatch
